\section{DESAFIO - Parte 2}

``O ambiente deve ser todo configurado através de gerenciador de
configuração, o que deverá ser entregue é um repositório git contendo
os arquivos de configuração que serão aplicados em uma máquina virtual
"zerada". Caso necessário descrever como executar o processo de
aplicação da configuração na máquina virtual. Ao final da tarefa e
execução do processo, deveremos ter um ambiente funcional; 

É recomendado que o repositório git seja entregue com commits
parciais, mostrando a evolução de seu código e pensamento. Caso
prefira nos informe um url de git público ou então compacte todos os
arquivos em um .tar.gz mantendo a pasta .git em conjunto; 

No ambiente deverá estar rodando uma aplicação node.js de exemplo,
conforme código abaixo. A versão do node.js deverá ser a última versão
LTS disponível em: https://nodejs.org/en/download/. A aplicação node
abaixo possui a dependência da biblioteca express. Garanta que seu
processo de bootstrap instale essa dependência ( última versão estável
disponível em: http://expressjs.com/ ) e rode o processo node em
background. De uma forma dinâmica garanta que seja criado uma
instância node para cada processador existente na máquina ( a máquina
poderá ter de 1 a 32 processadores );

Construa dentro de sua automação um processo de deploy e rollback
seguro e rápido da aplicação node. O deploy e rollback deverá garantir
a instalação das dependências node novas (caso sejam adicionadas ou
alteradas a versão de algum dependência por exemplo), deverá salvar a
versão antiga para possível rollback e reiniciar todos processos node
sem afetar a disponibilidade global da aplicação na máquina; 

A aplicação Node deverá ser acessado através de um Servidor Web
configurado como Proxy Reverso e que deverá intermediar as conexões
HTTP e HTTPS com os clientes e processos node. Um algoritmo de
balanceamento deve ser configurado para distribuir entre os N
processos node a carga recebida; 

A fim de garantir a disponibilidade do serviço, deverá estar funcional
uma monitoração do processo Node e Web para caso de falha, o mesmo
deve reiniciar ou subir novamente os serviços em caso de anomalia; 

Desenvolva um pequeno script que rode um teste de carga e demostre
qual o Throughput máximo que seu servidor consegue atingir; 

Desenvolva um script que parseie o log de acesso do servidor Web e
deverá rodar diariamente e enviar por e-mail um simples relatório, com
a frequência das requisições e o respectivo código de resposta (ex:5
/index.html 200); 

Por fim; rode o seu parser de log para os logs gerados pelo teste de
carga, garantindo que seu script terá performance mesmo em casos de
logs com milhares de acessos;

Nesta seção irei abordar em linhas gerais a ideia da minha
implementação, portanto não irei me deter em explicar o código, de
qualquer forma se houver alguma dúvida estou a disposição para
esclarecer qualquer ponto.''


\subsection{Introdução}
Pós a leitura do enunciado do desafio, comecei a selecionar quais
tecnologias iria utilizar, com exceção da NodeJS já pré
estabelecida. Dentro dos meus critérios para a seleção das tecnologias
estão: performance, atividade e correções de possíveis bugs, bem como
novas funcionalidades, ou seja, desenvolvimento ativo da tecnologia.
Como serviço http selecionei Nginx, sua característica de abrir
processos leve (threads) ao invés de processos já o torna
atraente. Como referência posso
citar~\footnote{\href{https://www.digitalocean.com/community/tutorials/apache-vs-nginx-practical-considerations}{https://www.digitalocean.com/community/tutorials/apache-vs-nginx-practical-considerations}}
com uma opinião, com tudo há diversos sites fazendo avaliações dos
prós e contras de Apache e Nginx, os mais utilizados. Os outros
quesitos também foram atendidos.

Ferramentas de automação, há diversas no mercado, como \emph{Chef, Puppet,
Ansible, Saltstack} etc. Selecionei a \emph{\textbf{Ansible}} devido a Linx Impulse
mencionar um diferencial na posição em aberto para Cloud Engineering.

Quanto aos scripts utilizei Bash e Python, no qual é nativo e
amplamente utilizado por muitas distribuições Linux, muito comum em
infraestrutura de nuvem.

Para simular meu ambiente, gerei três máquinas virtuais utilizando a
Virtual Machine Monitor (VMM) Qemu e libvirt. Criei uma segunda rede e
configurei Network Address Translation (NAT) pelo meu desktop para
chegar a Internet. Gerei uma imagem de Ubuntu 18.04, e apliquei as
mudanças em cima de alguns ``snapshoots copy on write''.

Serviços de inicialização da aplicação NodeJS foi utilizado o sistema
nativo, \emph{Systemd}, apesar de existirem outros como \emph{PM2},
recomendado para node, \emph{Supervisor} mais antigo, são boas
ferramentas, com tudo se estes processos (daemon) falham, a aplicação ficará
``órfã'' de monitoramento. Por outro lado o \emph{systemd} pode sofrer
a mesma coisa, porém ele é o processo 1, em caso de falha todo o
sistema operacional para, logo o confiamos, portanto é o mais robusto e
ainda nativo. Como gerente de tarefas para os scripts \emph{crontab}.


\subsection{Ambiente utilizado}
Em minha mini infra estrutura utilizei distribuições ArchLinux (pessoal), e
Ubuntu como servidor, segue versão:

\begin{verbatim}
Distributor ID:	Ubuntu
Description:	Ubuntu 18.04.4 LTS
Release:	18.04
Codename:	bionic

Linux webserver 4.15.0-76-generic #86-Ubuntu SMP
Fri Jan 17 17:24:28 UTC 2020 x86_64 x86_64 x86_64 GNU/Linux

VM's com 4 VCPU's e 4 GB de RAM cada.

ArchLinux
Ansible
ansible-playbook 2.9.4
python version = 3.8.1
\end{verbatim}


\subsection{Execução do gerenciador de configuração; \emph{Ansible}} \label{sec:playbook}
Primeiramente é necessário preencher as variáveis do ambiente do
Ansible: \emph{create\_user, copy\_local\_key, domain,
  release\_version}. Para envio de e-mail: \emph{email\_crontab,
  smtp\_server, smtp\_port, email\_from, email\_to, email\_login,
  email\_pass}.

Esta primeira implementação visa deixar flexível o lançamento,
nos quesitos: caminho de instalação da aplicação, domínio em que será
ofertado a aplicação, versão do primeiro lançamento e configurações
do usuário para enviar e-mail. Porém esta é uma versão beta no qual
identifiquei um problema, desescalar privilégio dos nodes workers,
no qual está estático. \textcolor{red}{\textbf{Não mude o nome do
    usuário}}, pois causará erro ao tentar desescalar. Ao corrigir
este bug, seria possível escolher o nome do usuário (caminho) onde
será instalado a aplicação. Outra melhora seria adicionar prefixo do
caminho, mais nome do usuário.

Outro problema é não tratamento de erro (com um try, catch) no script
python que, em caso de falha no módulo smtp, a aplicação dispara uma
exceção de código, onde o crontab irá disparar e-mail com o erro.

Um problema operacional são os dados sensíveis do usuário de e-mail,
geralmente em ambientes corporativos (nuvem) há um serviço de smtp
sem a necessidade de autenticação, para hosts específicos, o que
dispensaria este trecho. De qualquer forma o usuário que irá fazer o
deploy pode preencher os dados e manter em sigilo em sua máquina de
trabalho, com tudo, ficará exposto em texto puro no servidor. Acredito
que este quesito, (apesar de super relevante) para este desafio neste
momento dispense uma implementação com um mecanismo com hash ou até
mesmo obter credenciais de um serviço centralizado, como LDAP.

Este \emph{playbook} é dividido em duas partes, primeiro irá executar
os plays como \emph{root}, e depois como o usuário \emph{webserver}.
Em algum ponto é necessário escalar privilégios: como
reiniciar/recarregar o serviço da aplicação, e assim o play
o faz. Devido o primeiro contato com \emph{Ansible} não apliquei as
melhores práticas, porém são funcionais. Posso citar \emph{become}, para
escalar privilégios, pois ao tentar, falha. Isto ocorreu devido ao
pensamento em sempre deixar o mínimo de privilégios para usuários de
aplicação no servidor, o ideal é zero privilégios, mas sabemos que no
mundo real nem sempre é possível. Para permitir o mínimo de
privilégios, configurei o \emph{sudoers} para dois comandos bem
específicos, e devem ser executadas exatamente como está na regra,
(\emph{/bin/systemctl <start/reload> webserver.service}), sem por e nem
tirar, por definição do \emph{sudo}. O play com \emph{become} falha
como se não estive no \emph{sudoers}, provavelmente o Ansible está
executando \textbf{sem} o caminho absoluto, então não é aceito pelo
\emph{sudo}. Nestes casos implementei via script inline.\\

Executando o playbook:

$$ansible-playbook\ \ \ playbook.yml\ \ \ -l\ \ <server>$$

Ao final da execução deste playbook você terá um ambiente funcional
rodando a aplicação NodeJS de exemplo, e algumas rotas adicionais para
gerar uma carga artificial na aplicação.\\

Os passos de cada play é:
\begin{enumerate}
     \item Instalar os pacotes requeridos. A lista destes estão
     definida na variável do ambiente \emph{Ansible}.
     \item Criação de um usuário sem privilégios. Com exceção: iniciar e
     recarregar a aplicação node~\footnote{Isto se faz 
     necessário, devido a implementação através de
     clusterização}. Configurar este usuário para executar ssh no
   localhost, para o primeiro deploy, ao final será removido a chave
   ssh sem senha gerada pelo play.
    \item Cadastrar a chave pública (\emph{ssh}) da origem do comando
      para o servidor, possibilitando mais segurança no ambiente de deploy.
    \item Resiliência do serviço, através do \emph{systemd}. No qual
        uma vez que processo(s) da aplicação morre, ele automaticamente
        o reinicia, e mais script de monitoramento para tal. \label{i:systemd}
    \item Nginx com Proxy Reverso; balanceando a carga com
      \emph{round-robin} entre os possíveis 32 cores. Com um node
      worker para core, mais um Master. \label{i:nginx}
    \item Script que todos os dias à meia noite faz o parse do access
      log e gera uma tabela agregada por url (recurso), código e suas
      respectivas frequências de acesso.
    \item Instalação do ambiente node virtual Node Version Manager
      (NVM).
    \item Instalação do
      \href{https://github.com/alessandro11/desafio-2.git}{repositório}
      da aplicação node.
    \item Fácil e seguro mecanismo de \emph{deploy} e \emph{rollback}
      através do shipit.
\end{enumerate}


\subsubsection{Disponibilidade da aplicação}
O mecanismo nativo do systemd~\ref{i:systemd} permite a monitoração dos
processos, e em caso de falha e ou não resposta, os processos com falha recebem o sinal para
encerrar e iniciar novamente, e ainda em caso de falha do envio do
sinal ele permanece tentando com um intervalo de 5 sec
(configurável). Um pequeno engano no comentário do código (commit:
134c9d) fez a carga do servidor subir, pois o systemd incessantemente,
inciava o processo da aplicação node e em seguida ele ``morria'', pois
a plicação gerava uma exceção de código não trada, neste caso
comentários com '\#', ao invés de padrão C (//,
/\textasteriskcentered\textasteriskcentered).

E ainda bash script minuto a minuto verificando se o processo está rodando, e se
uma rota está retornando o código OK (200), se um dos casos não for
satisfeito o serviço é reiniciado.

O link para o script pode ser encontrado em:\\
\href{https://github.com/alessandro11/desafio-2/blob/master/scripts/health\_check.sh}{https://github.com/alessandro11/desafio-2/blob/master/scripts/health\_check.sh}


\subsubsection{Balanceamento de carga} \label{sec:balanceamento}
Além do mecanismo de balanceamento do item~\ref{i:nginx}, foi
implementado o serviço (daemon) utilizando cluster de workers
(processos nodes), em  outras palavras o daemon server.js
irá executar um processo a mais do que o número de cores do servidor,
este é o Master, quem delega para os demais workers, gera o arquivo de
de seu pid, logs etc. O Master também possui mecanismo de
balanceamento de carga, com tudo foi implementado balanceamento de
carga com o Nginx. O motivo da preferência por Nginx é uso de memória,
este é mais eficiente consequentemente utilizando menos memória. As
conexões https estão configuradas, porém é preciso 
gerar o certificado e apontar para o caminho do certificado
gerado.

A \emph{daemon} da aplicação foi construída de forma modular, ou seja,
\emph{server.js}, apenas inclui \emph{app.js} no qual poderia ter sua
própria estrutura hierarquia de middlewares, rotas etc, como é
mostrado no trecho de código abaixo:

Trecho de código retirado do fonte \href{https://github.com/alessandro11/desafio-2/blob/master/server.js}{server.js}:
\begin{verbatim}
...
100: } else {  // New worker running; load app
        var app = require('./app');

        app.listen(port, function () {
            console.log('Example app listening on port ' + port + '!');

            // Check if we are running as root
            if (process.getgid() === 0) {
                process.setgid('webserver');
                process.setuid('webserver');
            }
        });
112: }
...
\end{verbatim}

A configuração do upstream pode ser encontrado no seguinte link:\\
\href{https://github.com/alessandro11/ansible-desafio-2/blob/master/setup\_desafio-2/webserver.com}{https://github.com/alessandro11/ansible-desafio-2/blob/master/setup\_desafio-2/webserver.com}


\subsubsection{Throughput máximo}
A métrica utilizada em meu teste foi conexões por segundo
simultaneamente. O script abaixo gera conexões com o comando
\emph{curl ... -m 2 -w \%\{http\_code\} \%\{time\_starttransfer\}}, no
qual retorna código de respostas e tempo das requisições em no máximo
2 segundos.

Executando o teste (sem / no final do domínio):
$$./workload.sh\ <num\ req>\ <domain>$$
Caso não seja passado nenhum parâmetro, será executado 100 requisições
para o domínio \emph{webserver.com} (meu domínio local, em minha infra; note
que este domínio tem registro),

Um
\label{script:carga}
em bash executa requisições em paralelo ao servidor http. Como não há
carga no servidor pelo programa de exemplo, no qual apenas retorna
'Hello World!' código OK (200) (não gera carga). Então para gerá-la
artificialmente, gerei outras três rotas que aplica carga simbólica
no servidor. Cuja carga, é gerar contadores aleatórios, esperar por
alguns milissegundos. Podemos observar o servidor como idle entre um
sleep e outro como se estivesse esperando resposta de outro componente
do sistema, deste modo pude contrapor a primeira carga. Esta, resultou
em possíveis centenas de requisições simultâneas o que é irreal. Com a
carga artificial cheguei em média atingir 24 conexões simultâneas, com
apenas dois workers (duas VCPUS),

O script e resultados das análises, logs, estão no repositório em:\\
\href{https://github.com/alessandro11/desafio-2/tree/master/scripts}{https://github.com/alessandro11/desafio-2/tree/master/scripts}.


\subsubsection{Parser do log e relatório por e-mail}
Um script em python foi desenvolvido para está tarefa, este pode ser
encontrado no link ao final desta seção.

Execução do script:\\
(Caso queira receber por e-mail é necessário ter preenchido as
variáveis do Ansible, conforme seção~\ref{sec:playbook}.)

$$./log\_parser.py$$

A saída será uma tabela (no stdout) com as colunas URL, COD. e FREQ, este é o
número de requisições nas últimas 24h, agregadas por código e url.

Explorando o algoritmo eficiente de dicionário do python, é possível
agregar os dados de logs com milhares de linhas, em uns dos testes
fora executado em um arquivo com mais 50k linhas. Os resultados e logs
podem ser obtidos no repositório:
\href{https://github.com/alessandro11/linx-challenge/tree/master/results}{resultados}.

A entrada abaixo foi gerada no \emph{crontab} do root (é necessário
acesso ao \textit{/var/log/nginx/access\_webserver.log}) para ser
executado todos os dias à meia-noite.

\begin{verbatim}
0 0    * * *    /home/webserver/current/scripts/log\_parser.py
\end{verbatim}

O fonte do script pode ser encontrado no link:\\
\href{https://github.com/alessandro11/desafio-2/blob/master/scripts/log\_parser.py}{https://github.com/alessandro11/desafio-2/blob/master/scripts/log\_parser.py} \label{lnk:logparser}


\subsubsection{Deploy e Rollback}
Este mecanismo foi implementado através de dois pacotes nodes obtidos com
\emph{npm}: shipit-cli@5.1.0 e shipit-deploy@5.1.0. \textbf{Para fazer o depoy,
é necessário executar o git push das mudanças no repositório que se deseja lançar}.

Primeiramente você deve editar o arquivo
\href{https://github.com/alessandro11/desafio-2/blob/master/shipitfile.js}{shipitfile.js}
localizado na raiz do repositório, alterando as seguintes variáveis:

Home do usuário onde foi clonado o repositório no servidor. \textbf{Caso não
tenha alterado o playbook não é necessário modificar.}\\

\color{red}
Aponte para um caminho que não seja a raiz do repositório,
Este cuidado deve ser tomado se for setado a variável para limpar o
diretório do deploy. Ou seja, em minha configuração, será clonado
para \textit{/home/webserver/desafio-2}, e o deploy ocorrerá em
\textit{/home/webserver/releases}.

\color{black}
\begin{verbatim}
...
   default: {
       deployTo: /home/webserver/
       ...
  }
...
\end{verbatim}

Aponte para o branch ou tag que deseja fazer deploy,
\begin{verbatim}
...
banch: 'master',
ou
tag: 'rc-0.0.1',
...
\end{verbatim}

Usuário e domínio/IP do servidor que será feio o deploy.
\begin{verbatim}
...
    production: {
        servers: 'webserver@webserver.com',
    },
...
\end{verbatim}

É possível fazer uma sobre escrita das configurações padrões,
como \emph{branch} por exemplo, caso a configuração \emph{production}
possua um branch chamado production:
\begin{verbatim}
...
    production: {
        servers: 'webserver@webserver.com',
        branch: 'production',
    },
...

...
    development: {
        servers: 'webserver@webserverdev.com',
        branch: 'dev',
    },
...
\end{verbatim}

Aponte para o caminho da \$HOME onde a aplicação está. Este é o
trigger que é gerado após o deploy com sucesso, então possíveis novos
pacotes serão instalados, bem como o serviço irá recarregar os workers
com as novas mudanças.

\begin{verbatim}
    await shipit.remote('cd current; \
         /home/webserver/.nvm/nvm-exec npm install; \
         sudo /bin/systemctl reload webserver.service')
\end{verbatim}

\textcolor{red}{Os serviços foram gerados para procurar a home do
  usuário em /home, portanto não altere o prefixo.}

\subsubsection{Deploy}
Execute o comando abaixo a partir do diretório do
\emph{shipitfile.js} (\textbf{a \emph{branch} ou \emph{tag}} deve
estar no repositório remoto):

$$npx\ \ shipit\ \ production\ \ deploy$$

A chave ssh será usado para acessar o servidor e executar o
deploy. Será extraído o último commit da branch ou tag apontado pelo
shipitfile e clonado para o diretório
\textit{<\textit{deployTo:}>/releases} no servidor remoto. Você
encontrará os diretórios com o último commit do deploy, o nome dos
diretórios são um timestamp. Um link simbólico chamado
\textit{<deployTo:>/current} aponta para o diretório do deploy atual
em \textit{<deployTo:>/releases/20200208160632>} e.g. 


\subsubsection{Rollback}
Execute o comando abaixo a partir do diretório do
\emph{shipitfile.js}:

$$npx\ \ shipit\ \ production\ \ rollback$$

Tudo que ocorrerá é apontar o link simbólico para o deploy anterior, e
recarregar os nodes workers, recarregando a aplicação. Está pré
configurado para armazenar os últimos dez deploys, isto pode ser
configurado no shipitfile.


